"""AI client for generating news summaries with provider abstraction."""

import json
import os
import re
from abc import ABC, abstractmethod
from typing import Optional

from aws_lambda_powertools import Logger
from groq import Groq


class AIClient(ABC):
    """Abstract base class for AI providers."""

    @abstractmethod
    def evaluate_impact(self, news_batch: list[dict]) -> list[dict]:
        """Rate global market impact of news (1-10 scale)."""
        pass

    @abstractmethod
    def generate_news_summary(self, news_items: list[dict], language: str = "kk") -> str:
        """Generate a formatted news summary in the specified language."""
        pass

    def _get_no_news_message(self, language: str) -> str:
        """Get 'no news' fallback message."""
        messages = {
            "kk": "üì≠ –ë“Ø–≥—ñ–Ω –∂–∞“£–∞–ª—ã“õ –∂–æ“õ.",
            "ru": "üì≠ –°–µ–≥–æ–¥–Ω—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç.",
            "en": "üì≠ No news today."
        }
        return messages.get(language, messages["en"])


class GroqAIClient(AIClient):
    """Groq AI provider implementation with chunking and resilient parsing."""

    def __init__(self, api_key: Optional[str] = None):
        """Initialize Groq client."""
        self.api_key = api_key or os.environ.get("GROQ_API_KEY")
        if not self.api_key:
            raise ValueError("GROQ_API_KEY is required")
        self.client = Groq(api_key=self.api_key)
        self.logger = Logger()

    def evaluate_impact(self, news_batch: list[dict]) -> list[dict]:
        """Evaluate market impact processing news in safe chunks."""
        if not news_batch:
            return []

        chunk_size = 20
        all_processed_news = []
        
        for i in range(0, len(news_batch), chunk_size):
            chunk = news_batch[i:i + chunk_size]
            try:
                processed_chunk = self._process_scoring_chunk(chunk)
                all_processed_news.extend(processed_chunk)
            except Exception as e:
                self.logger.error(f"Failed to process scoring chunk {i}", exc_info=True)
                for item in chunk:
                    item["impact_score"] = 5
                    item["reason"] = f"Scoring error: {str(e)}"
                all_processed_news.extend(chunk)
                
        return all_processed_news

    def _process_scoring_chunk(self, chunk: list[dict]) -> list[dict]:
        """Send a single chunk to LLM and parse the JSON array response."""
        payload = [{"id": n["id"], "title": n["title"]} for n in chunk]
        
        prompt = (
            "Rate global IT market impact (1-10):\n"
            "- 10: Massive investments ($100B+), AI breakthroughs, critical security\n"
            "- 5: Major updates, new features\n"
            "- 1: Minor bug fixes\n"
            "Respond ONLY with valid JSON array of objects: "
            '[{"id": int, "impact_score": int, "reason": "short string"}]\n'
            f"Data: {json.dumps(payload, ensure_ascii=False)}"
        )

        response = self.client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=2000,
        )
        
        content = response.choices[0].message.content.strip()
        
        # Extract JSON using Regex
        json_match = re.search(r'\[.*\]', content, re.DOTALL)
        if json_match:
            content = json_match.group(0)
        
        # Safely parse JSON
        try:
            scores = json.loads(content)
        except json.JSONDecodeError:
            self.logger.error(f"LLM returned invalid JSON: {content}")
            scores = []
        
        # Merge scores back to chunk
        for news in chunk:
            score_data = next((item for item in scores if item.get("id") == news.get("id")), None)
            if score_data:
                news["impact_score"] = score_data.get("impact_score", 0)
                news["reason"] = score_data.get("reason", "No reason")
            else:
                news["impact_score"] = 5
                news["reason"] = "Not returned by AI"
                
        return chunk

    def generate_news_summary(self, news_items: list[dict], language: str = "kk") -> str:
        """Generate formatted news summary."""
        if not news_items:
            return self._get_no_news_message(language)

        news_text = "\n\n".join([
            f"–¢–∞“õ—ã—Ä—ã–ø: {item['title']}\n–°–∏–ø–∞—Ç—Ç–∞–º–∞: {item.get('summary', '–ñ–æ“õ')}"
            for item in news_items[:3]
        ])

        prompt = self._build_prompt(news_text, language)

        try:
            response = self.client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {
                        "role": "system",
                        "content": "–°—ñ–∑ IT –∂”ô–Ω–µ developer –∂–∞“£–∞–ª—ã“õ—Ç–∞—Ä—ã–Ω “õ–∞–∑–∞“õ —Ç—ñ–ª—ñ–Ω–¥–µ –∂–∞–∑–∞—Ç—ã–Ω —Ç–µ—Ö–Ω–∏–∫–∞–ª—ã“õ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç—Å—ñ–∑. –¢–µ–∫ ”©—Ç–µ –º–∞“£—ã–∑–¥—ã –∂”ô–Ω–µ –∞–∫—Ç—É–∞–ª–¥—ã –∂–∞“£–∞–ª—ã“õ—Ç–∞—Ä–¥—ã —Ç–∞“£–¥–∞“£—ã–∑."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=1200,
            )

            return response.choices[0].message.content

        except Exception as e:
            self.logger.error(f"Summary generation failed: {e}")
            return f"‚ùå “ö–∞—Ç–µ –æ—Ä—ã–Ω –∞–ª–¥—ã: {str(e)}"

    def _build_prompt(self, news_text: str, language: str) -> str:
        """Construct the prompt template based on language."""
        if language == "kk":
            return f"""–ú—ã–Ω–∞ IT –∂–∞“£–∞–ª—ã“õ—Ç–∞—Ä—ã–Ω “õ–∞–∑–∞“õ —Ç—ñ–ª—ñ–Ω–¥–µ “õ—ã—Å“õ–∞—à–∞ –º–∞–∑–º“±–Ω–¥–∞“£—ã–∑. 
            
            –ú–ê“¢–´–ó–î–´ –§–û–†–ú–ê–¢–¢–ê–£ –ï–†–ï–ñ–ï–õ–ï–†–Ü:
            1. –¢–µ–∫ 3 –µ“£ –º–∞“£—ã–∑–¥—ã –∂”ô–Ω–µ “õ—ã–∑—ã“õ—Ç—ã –∂–∞“£–∞–ª—ã“õ—Ç—ã —Ç–∞“£–¥–∞“£—ã–∑
            2. Developer –∂”ô–Ω–µ IT-“õ–∞ “õ–∞—Ç—ã—Å—Ç—ã –º–∞–∑–º“±–Ω“ì–∞ –±–∞—Å—ã–º–¥—ã“õ –±–µ—Ä—ñ“£—ñ–∑
            3. –°—ñ–ª—Ç–µ–º–µ–ª–µ—Ä–¥—ñ “ö–û–°–ü–ê“¢–´–ó (–æ–ª–∞—Ä–¥—ã –∂—ñ–±–µ—Ä–º–µ“£—ñ–∑)
            4. –¶–∏—Ç–∞—Ç–∞ (blockquote) —Ñ–æ—Ä–º–∞—Ç—ã–Ω –ø–∞–π–¥–∞–ª–∞–Ω—ã“£—ã–∑: <blockquote>–°–∏–ø–∞—Ç—Ç–∞–º–∞</blockquote> - —Å–∏–ø–∞—Ç—Ç–∞–º–∞–Ω—ã HTML —Ç–µ–≥—Ç–µ—Ä—ñ–º–µ–Ω –æ—Ä–∞–ø “õ–æ–π—ã“£—ã–∑

            –§–æ—Ä–º–∞—Ç—ã:
            üî•<b>–ö“Ø–Ω–Ω—ñ“£ IT –∂–∞“£–∞–ª—ã“õ—Ç–∞—Ä—ã</b>

            <b>[Bold —Ç–∞“õ—ã—Ä—ã–ø - “õ—ã—Å“õ–∞ –∂”ô–Ω–µ –Ω–∞“õ—Ç—ã]</b>
            <blockquote>“ö—ã—Å“õ–∞—à–∞ —Å–∏–ø–∞—Ç—Ç–∞–º–∞ 2-3 —Å”©–π–ª–µ–º. –ù–µ–≥–µ –±“±–ª –º–∞“£—ã–∑–¥—ã? ”ò—Å–µ—Ä—ñ “õ–∞–Ω–¥–∞–π?</blockquote>

            <b>[Bold —Ç–∞“õ—ã—Ä—ã–ø]</b>
            <blockquote>“ö—ã—Å“õ–∞—à–∞ —Å–∏–ø–∞—Ç—Ç–∞–º–∞</blockquote>

            <b>[Bold —Ç–∞“õ—ã—Ä—ã–ø]</b>  
            <blockquote>“ö—ã—Å“õ–∞—à–∞ —Å–∏–ø–∞—Ç—Ç–∞–º–∞</blockquote>

            –ñ–∞“£–∞–ª—ã“õ—Ç–∞—Ä:
            {news_text}

            –ï—Å—Ç–µ —Å–∞“õ—Ç–∞“£—ã–∑:
            - Emoji –ø–∞–π–¥–∞–ª–∞–Ω—ã“£—ã–∑ (üöÄ üî• üíª ü§ñ üîí ‚ö° etc)
            - –¢–∞“õ—ã—Ä—ã–ø—Ç—ã BOLD –µ—Ç—ñ“£—ñ–∑: <b>–¢–∞“õ—ã—Ä—ã–ø</b>
            - –°–∏–ø–∞—Ç—Ç–∞–º–∞–Ω—ã blockquote –µ—Ç—ñ“£—ñ–∑: <blockquote>–°–∏–ø–∞—Ç—Ç–∞–º–∞</blockquote>
            - –°—ñ–ª—Ç–µ–º–µ–ª–µ—Ä–¥—ñ “ö–û–°–ü–ê“¢–´–ó
            - –¢–µ–∫ –µ“£ –º–∞“£—ã–∑–¥—ã 3 –∂–∞“£–∞–ª—ã“õ"""
        else:
            return f"Summarize these IT news in {language}:\n{news_text}"


def create_ai_client(provider: str = "groq", api_key: Optional[str] = None) -> AIClient:
    """Factory function to create AI client based on provider."""
    if provider.lower() == "groq":
        return GroqAIClient(api_key=api_key)
    else:
        raise ValueError(f"Unsupported AI provider: {provider}. Available: 'groq'")